"""
Code adapted from https://github.com/huggingface/transformers
"""
import logging
import sys
import datasets
import numpy as np
import transformers
from transformers import (
    AutoConfig,
    AutoModelForTokenClassification,
    AutoTokenizer,
    DataCollatorForTokenClassification,
    HfArgumentParser,
    PretrainedConfig,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.utils import check_min_version
from transformers.utils.versions import require_version
from dataclasses import dataclass, field
from datasets import ClassLabel, load_dataset
from typing import Optional


check_min_version("4.25.0.dev0")
require_version(
    "datasets>=1.8.0",
    "datasets must be version 1.8.0 or greater")
logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    model: str = field(
        metadata={"help": "Model file"}
    )
    config_name: Optional[str] = field(
        default=None,
        metadata={"help": "Pretrained config name or path"}
    )
    tokenizer_name: Optional[str] = field(
        default=None,
        metadata={"help": "Pretrained tokenizer name or path"}
    )


@dataclass
class DataTrainingArguments:
    task_name: Optional[str] = field(
        default="ner",
        metadata={"help": "The name of the task (ner, pos...)."}
    )
    dataset_name: Optional[str] = field(
        default=None,
        metadata={"help": "The name of the dataset to use."}
    )
    dataset_config_name: Optional[str] = field(
        default=None,
        metadata={"help": "The configuration name of the dataset to use."}
    )
    train_file: Optional[str] = field(
        default=None,
        metadata={"help": "The input training data file (JSON file)."}
    )
    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input test data file to predict on."},
    )
    output_file: Optional[str] = field(
        default="predictions.txt",
        metadata={"help": "Output file with predictions"}
    )
    overwrite_cache: bool = field(
        default=False,
        metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for the preprocessing."},
    )
    max_seq_length: int = field(
        default=128,
        metadata={
            "help": (
                "The maximum total input sequence length after tokenization. "
                "If set, sequences longer than this will be truncated, "
                "sequences shorter will be padded."
            )
        },
    )
    pad_to_max_length: bool = field(
        default=False,
        metadata={
            "help": (
                "Whether to pad all samples to model maximum sentence "
                "length. If False, will pad the samples dynamically when "
                "batching to the maximum length in the batch. More "
                "efficient on GPU but very bad for TPU."
            )
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the "
                "number of training examples to this value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the "
                "number of evaluation examples to this value if set."
            )
        },
    )
    max_predict_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the "
                "number of prediction examples to this value if set."
            )
        },
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": (
                "Whether to put the label for one word on all tokens of "
                "generated by that word or just on the one (in which case "
                "the other tokens will have a padding index)."
            )
        },
    )
    return_entity_level_metrics: bool = field(
        default=False,
        metadata={
            "help": (
                "Whether to return all the entity levels during "
                "evaluation or just the overall ones."
            )
        },
    )

    def __post_init__(self):
        if self.dataset_name is None and self.train_file is None:
            raise ValueError("Need either a dataset name or a training file.")
        else:
            if self.train_file is not None:
                extension = self.train_file.split(".")[-1]
                assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."  # noqa
        self.task_name = self.task_name.lower()


def get_label_list(labels):
    unique_labels = set()
    for label in labels:
        unique_labels = unique_labels | set(label)
    label_list = list(unique_labels)
    label_list.sort()
    return label_list


parser = HfArgumentParser((ModelArguments, DataTrainingArguments))
model_args, data_args = parser.parse_args_into_dataclasses()
training_args = TrainingArguments(
    output_dir=".",
    per_gpu_train_batch_size=32,
    do_predict=True,
    disable_tqdm=False,
)

# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)
log_level = logging.ERROR
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)
transformers.utils.logging.enable_default_handler()
transformers.utils.logging.enable_explicit_format()
datasets.disable_progress_bar()

# Set seed before initializing model.
set_seed(training_args.seed)

data_files = {}
if data_args.train_file is not None:
    data_files["train"] = data_args.train_file
if data_args.test_file is not None:
    data_files["test"] = data_args.test_file
extension = data_args.train_file.split(".")[-1]

raw_datasets = load_dataset(
    extension, data_files=data_files, use_auth_token=False, cache_dir=None)

column_names = raw_datasets["test"].column_names
features = raw_datasets["test"].features
text_column_name = "tokens"
label_column_name = column_names[1]

labels_are_int = isinstance(features[label_column_name].feature, ClassLabel)
if labels_are_int:
    label_list = features[label_column_name].feature.names
    label_to_id = {i: i for i in range(len(label_list))}
else:
    label_list = get_label_list(raw_datasets["train"][label_column_name])
    label_to_id = {l: i for i, l in enumerate(label_list)}

num_labels = len(label_list)

config = AutoConfig.from_pretrained(
    model_args.config_name if model_args.config_name else model_args.model,
    num_labels=num_labels,
    finetuning_task=data_args.task_name,
    revision="main",
    use_auth_token=False,
)

tokenizer_name = (
    model_args.tokenizer_name
    if model_args.tokenizer_name else model_args.model
)
if config.model_type in {"bloom", "gpt2", "roberta"}:
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_name,
        use_fast=True,
        revision="main",
        use_auth_token=False,
        add_prefix_space=True,
    )
else:
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_name,
        use_fast=True,
        revision="main",
        use_auth_token=False,
    )

model = AutoModelForTokenClassification.from_pretrained(
    model_args.model,
    from_tf=bool(".ckpt" in model_args.model),
    config=config,
    revision="main",
    use_auth_token=False,
)

if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:
    if list(sorted(model.config.label2id.keys())) == list(sorted(label_list)):
        # Reorganize `label_list` to match the ordering of the model.
        if labels_are_int:
            label_to_id = {
                i: int(model.config.label2id[l])
                for i, l in enumerate(label_list)
            }
            label_list = [model.config.id2label[i] for i in range(num_labels)]
        else:
            label_list = [model.config.id2label[i] for i in range(num_labels)]
            label_to_id = {l: i for i, l in enumerate(label_list)}
    else:
        logger.warning(
            "Your model seems to have been trained with labels, but they "
            "don't match the dataset: ",
            f"model labels: {list(sorted(model.config.label2id.keys()))}, "
            "dataset labels:"
            f" {list(sorted(label_list))}.\n"
            "Ignoring the model labels as a result.",
        )

model.config.label2id = {l: i for i, l in enumerate(label_list)}
model.config.id2label = {i: l for i, l in enumerate(label_list)}

b_to_i_label = []
for idx, label in enumerate(label_list):
    if label.startswith("B-") and label.replace("B-", "I-") in label_list:
        b_to_i_label.append(label_list.index(label.replace("B-", "I-")))
    else:
        b_to_i_label.append(idx)

padding = "max_length" if data_args.pad_to_max_length else False


def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples[text_column_name],
        padding=padding,
        truncation=True,
        max_length=data_args.max_seq_length,
        is_split_into_words=True,
    )
    labels = []
    for i, label in enumerate(examples[label_column_name]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label_to_id[label[word_idx]])
            else:
                if data_args.label_all_tokens:
                    label_ids.append(
                        b_to_i_label[label_to_id[label[word_idx]]]
                    )
                else:
                    label_ids.append(-100)
            previous_word_idx = word_idx

        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs


predict_dataset = raw_datasets["test"]
if data_args.max_predict_samples is not None:
    max_predict_samples = min(
        len(predict_dataset), data_args.max_predict_samples
    )
    predict_dataset = predict_dataset.select(
        range(max_predict_samples)
    )
with training_args.main_process_first(desc="prediction dataset map"):
    predict_dataset = predict_dataset.map(
        tokenize_and_align_labels,
        batched=True,
        num_proc=data_args.preprocessing_num_workers,
        load_from_cache_file=not data_args.overwrite_cache,
        desc="Running tokenizer on prediction dataset",
    )

data_collator = DataCollatorForTokenClassification(
    tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=None,
    eval_dataset=None,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

predictions, labels, metrics = trainer.predict(
    predict_dataset, metric_key_prefix="predict")
predictions = np.argmax(predictions, axis=2)

true_predictions = [
    [label_list[pr] for (pr, lab) in zip(prediction, label) if lab != -100]
    for prediction, label in zip(predictions, labels)
]

sentences = predict_dataset.to_dict()['tokens']
with open(data_args.output_file, 'w') as ofile:
    for i, sentence in enumerate(sentences):
        for j, token in enumerate(sentence):
            print('{} {}'.format(token, true_predictions[i][j]), file=ofile)
        print(file=ofile)
